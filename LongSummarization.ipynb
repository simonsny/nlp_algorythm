{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installing Transformers and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (4.5.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from transformers) (4.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: requests in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: dataclasses in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: packaging in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: sacremoses in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/simon/anaconda3/envs/nlp_becode/lib/python3.6/site-packages (from sacremoses->transformers) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/simon/anaconda3/envs/nlp_becode\n",
      "\n",
      "  added / updated specs:\n",
      "    - beautifulsoup4\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    beautifulsoup4-4.9.3       |     pyhb0f4dca_0          87 KB  anaconda\n",
      "    soupsieve-2.0.1            |             py_0          33 KB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         120 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  beautifulsoup4     anaconda/noarch::beautifulsoup4-4.9.3-pyhb0f4dca_0\n",
      "  soupsieve          anaconda/noarch::soupsieve-2.0.1-py_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2020.12.~ --> anaconda::ca-certificates-2020.10.14-0\n",
      "  certifi            conda-forge::certifi-2020.12.5-py36h5~ --> anaconda::certifi-2020.6.20-py36_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "beautifulsoup4-4.9.3 | 87 KB     | ##################################### | 100% \n",
      "soupsieve-2.0.1      | 33 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda beautifulsoup4 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Summarization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get Blog Post from Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://towardsdatascience.com/a-bayesian-take-on-model-regularization-9356116b6457\"\n",
    "#URL = \"https://hackernoon.com/will-the-game-stop-with-gamestop-or-is-this-just-the-beginning-2j1x32aa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "results = soup.find_all(['h1', 'p'])\n",
    "text = [result.text for result in results]\n",
    "ARTICLE = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sign in A Bayesian Take On Model Regularization Ryan Sander Feb 1·10 min read I’m currently reading “How We Learn” by Stanislas Dehaene. First off, I cannot recommend this book enough to anyone interested in learning, teaching, or AI. One of the main themes of this book is explaining the neurological and psychological bases of why humans are so good at learning things quickly and with great sample-efficiency, i.e. given only a limited amount of experience¹. One of Dehaene’s main arguments of why humans can learn so effectively is because we are able to reduce the complexity of models we formulate of the world. In accordance with the principle of Occam’s Razor², we find the simplest model possible that explains the data we experience, rather than opting for more complicated models. But why do we do this, even from birth¹? One argument is that, contrary to the frequentist view in child psychology (the belief that babies learn solely through their experiences), we are already imparted with prior beliefs about the world when we are born¹. This notion of simplified model selection has a common name in the field of machine learning: model regularization. In this article, we’ll talk about regularization from a Bayesian perspective. What’s one way we can control the complexity of the models we learn from observations? We can do this by placing a prior on our distribution of models. Before we show this, let’s briefly go over regularization, in this case, analytic regularization for supervised learning. Background on Regularization In machine learning, regularization, or model complexity control, is an essential and common practice to ensure that a model attains high out-of-sample performance, even if the distribution of out-of-sample data (test/validation data) differs significantly from the distribution of in-sample data (training data). In essence, the model must balance having a small empirical loss (how “wrong” it is on the data it is given) with a small regularization loss (how complicated the model is). In supervised learning, regularization is usually accomplished via L2 (Ridge)⁸, L1 (Lasso)⁷, or L2/L1 (ElasticNet)⁹ regularization. For neural networks, there are also techniques such as Drop-out³ or Early Stopping⁴. For now, we will focus on analytical regularization techniques, since their Bayesian interpretation is more well-defined. These techniques are summarized below. Let’s start by defining our dataset and parameters: Next, for a given supervised learning problem in which we wish to minimize a loss function (e.g. Mean-Squared Error): Then we have the following objectives for each type of analytical supervised regularization techniques: Below is a comparative plot showing the norm plots of Lasso, ElasticNet, and Ridge regularization, each drawn over a unit sphere. The code to generate this plot can be found in the Appendix. In summary, these regularization techniques accomplish different objectives for controlling the complexity of our models. In the next section, we will derive these regularized objectives by imposing a prior belief (in the form of a probability distribution) on our model parameters, thus directly making the link between prior beliefs and model regularization. Model Regularization as a Prior Belief on Models Let’s dive deeper into the probabilistic and optimization theory behind implementing regularization through a prior belief in our model parameters. Specifically, we will demonstrate that: We will analyze these claims for regression problems, but they extend to other supervised learning tasks, such as classification, as well. We’ll focus on rigorously presenting the mathematics behind these claims (you can also find these derivations in this document here). Let’s dive in! L2 (Ridge) Regularization as a Multivariate Gaussian Prior Suppose we again have the same set of observations D and a parameter vector w that we want to optimize in order to best make predictions on samples from D: Using the Maximum a Posteriori (MAP) rule, we can show that the mean and mode of the posterior distribution of w is the solution for ridge regression when we invoke a Gaussian prior distribution on w. We first invoke Bayes’ Rule: We now define our prior and observation model distributions, with the following assumptions: a. Prior Model (distribution over parameters w): b. Observation Model (conditional distribution of observations D conditioned on parameters): Now, let’s substitute these expressions into Bayes’ Rule: To derive our L2 regularized estimator, we now use the MAP rule and the negative log-likelihood function to transform this expression over products into an expression over sums. This operation is permissible because: (i) The logarithm of the objective is a strictly monotonic transformation of the likelihood function, and thus taking the maximum argument of the log-likelihood function will preserve the cardinality of the likelihood objective and yield the same maximizing argument. (ii) The maximizing argument of any objective J(θ) is the same as the minimizing argument of the negative objective -J(θ). Therefore, the argument w* that maximizes the log-likelihood will minimize the negative log-likelihood: Substituting our posterior distribution into our expression for negative log-likelihood: Since logarithms transform products into sums, we can decompose this logarithm of products into a sum of summation terms that depend on our parameters w, along with constants that do not depend on w. We can then use logarithm rules to simplify this expression. Removing terms that don’t depend on our parameters w, multiplying the expression by a constant σ², we obtain: The setting of λ = σ² / τ yields that our MAP estimator is also the estimator obtained via ridge regression (when our data is centered around 0): This corresponds exactly to our ridge objective (for regression) above! In closed-form, this yields the normal equations: Where the λI term is used to ensure that the matrix to be inverted (X^T X) is positive semi-definite, and therefore invertible. Therefore, placing a Multivariate Gaussian prior on our parameters is equivalent to regularizing our parameters with an L2-norm penalty. We’ll now examine a similar case with a Laplace prior. Suppose again that we have a dataset of observations D and a parameter vector w that we want to optimize in order to best make predictions on samples from D: Again using the MAP rule, we can show that the mean and mode of the posterior distribution of w is the solution for LASSO regression when we invoke a Gaussian prior distribution on w. We first invoke Bayes’ Rule: We now define our prior and observation model distributions, with the following assumptions: a. Prior Model (distribution over parameters w): b. Observation Model (Same distribution as above): Repeating the same steps as above (referenced below): Here is the corresponding derivation for Lasso: (Note that in the last step, we set p = 1, q = 1, and λ = α_1 / α_2.) As before, we have derived our Lasso objective (for regression) as described in the section above! Why Does This Matter? Though it may seem like all we did was invoke some tricks with optimization, logarithms, and probability distributions, the significance of the above derivations may be best understood in the context of the aforementioned novel “How We Learn”. If I, a human being, am learning a model of the world, why do I tend towards the simplest model that explains my observations¹? One reason for this is because our brain creates “prior beliefs” over the models we learn even before we learn them, which we take into account when we learn from experience. Rather than learning these models only from experience, however, we use experience to update our previous beliefs of these models. By placing a prior belief that the models we learn must be as simple as possible, we are able to control the complexity of the models we learn even before we learn them! This is exactly what we have done with our analytic derivations above: by placing a prior belief on the distribution of our model parameters (i.e. “the model parameters w are normally-distributed”) we are able to directly shape how complex these models are. Summary In this article, we introduced the ideas of model complexity and regularization, and how these concepts relate to the idea of prior beliefs. We made the claim that we can control the complexity of a model, i.e. regularize it, by setting a prior belief on our distribution of parameters. We then briefly introduced some common analytical, supervised regularization techniques (Ridge, Lasso, and ElasticNet regression). We then showed how we can derive the objective functions for Ridge and Lasso regularization using Multivariate Gaussian and Laplace prior distributions, respectively. Finally, we talked about why these results are significant not only for machine learning, but for psychology as well. Thanks for reading :) Please follow me for more articles in reinforcement learning, computer vision, programming, and optimization! Also, a huge thank you to CODECOGS for their online equation rendering tool! It’s very helpful and easy to use if you want to render mathematics in your Medium articles. References [1] Dehaene, Stanislas. How we learn: Why brains learn better than any machine… for now. Penguin, 2020. [2] Rasmussen, Carl Edward, and Zoubin Ghahramani. “Occam’s razor.” Advances in neural information processing systems (2001): 294–300. [3] Srivastava, Nitish, et al. “Dropout: a simple way to prevent neural networks from overfitting.” The journal of machine learning research 15.1 (2014): 1929–1958. [4] Caruana, Rich, Steve Lawrence, and Lee Giles. “Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping.” Advances in neural information processing systems (2001): 402–408. [5] Tibshirani, Robert. “Regression shrinkage and selection via the lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58.1 (1996): 267–288. [6] Calvetti, Daniela, and Lothar Reichel. “Tikhonov regularization of large linear problems.” BIT Numerical Mathematics 43.2 (2003): 263–283. [7] Tibshirani, Robert. “Regression shrinkage and selection via the lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58.1 (1996): 267–288. [8] Hoerl, Arthur E., and Robert W. Kennard. “Ridge regression: Biased estimation for nonorthogonal problems.” Technometrics 12.1 (1970): 55–67. [9] Zou, Hui, and Trevor Hastie. “Regularization and variable selection via the elastic net.” Journal of the royal statistical society: series B (statistical methodology) 67.2 (2005): 301–320. Appendix Code to generate plots (adapted from this Stack OverFlow post). Graduate Research Assistant @ MIT CSAIL, Tutor, Dark Roast Coffee Drinker, GitHub: https://github.com/rmsander/, LinkedIn: https://www.linkedin.com/in/rmsander 319  1  Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss.\\xa0Take a look.  By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices. Check your inboxMedium sent you an email at  to complete your subscription. 319\\xa0 319  1  Your home for data science. A Medium publication sharing concepts, ideas and codes. About Help Legal Get the Medium app\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Will The Game Stop with Gamestop Or Is This Just The Beginning? Crypto, Markets, Trading The GameStop squeeze on short-sellers is an extraordinary event in markets, where at face value, retail traders and investors have worked together in an attempt to put some of the largest wall street institutions out of business. The events can be interpreted with many viable lenses and there are ironies baked in that are pure serendipity. There has been a centrally controlled game in the global financial system in which insiders benefited while outsiders got hurt that comes to a head with a company called GameStop. The broking firm of most of the retail side of this warfare ‘RobinHood’ is literally stealing from its poor, retail investors to give to its rich, capital backers. One of the historical realities of this game has been that macro-investing – the sages of not only portfolio management, but often also sophisticated social and cultural figures – have had a hard time making money in markets now for decades. With government policy dictating markets in greater degrees since the mid-90’s, the politically connected have been more likely to survive while the sophisticated and capable have been more likely to struggle. Quantitative and high-frequency trading has entered and changed the structure of global markets with this change invited through central banking policy characterised by moral hazard. Commerce and politics has grown closer and so commercial survival has demanded a political edge. Although what I really want to talk about is how market structure has changed since 1995 and how this has contributed to the situation here, a little bit of the history is first required. Drinks on Alan The tequila crises was one of the first events in late 1994, early 1995 in which insiders were bailed out and the might of the U.S. governments political capital was used to organise $50-$80 billion USD in capital for political insiders capital at risk in Mexico.  The big moral hazard there was ex-Goldman Sachs Treasury Secretary Robert Rubin using the state ‘Exchange Stabilisation Fund’ to support a Mexican bailout that protected Goldman Sachs investments. It was Alan Greenspan presiding over the Federal Reserve at the time, meeting with the men in power with sharp suits and important titles. It probably even felt like the right thing to do. This ‘Greenspan put’ as his actions became known in the investing world, fundamentally changed the structure of markets and gave enormous knowledge and power to the politically connected.  (This concept, of a ‘Greenspan put’ refers to the willingness of the Federal Reserve chairman to protect asset prices. A put is insurance against declining asset prices in which the put seller can lose big.) Meet Victor Niederhoffer. Victor Niederhoffer is one of the most successful people alive today.  Niederhoffer studied statistics and economics at Harvard and the University of Chicago, was a finance professor at the University of California and while at college he co-founded an investment bank. Having never picked up a racquet before Harvard, Niederhoffer won the squash national junior title a year later, graduated as the national intercollegiate champion, won the U.S. nationals 5 times and defeated one of the greatest players in the history of the sport in the North American Open, becoming a member of the squash hall of fame. His investment record is tremendous, with an average of a 35% return annualised and once working for fellow legend of funds management George Soros. When it came time for his son to learn the family business, Soros sent his son to Niederhoffer. Niederhoffers books ‘Practical Speculation’ and ‘The Education of a Speculator’ are essential reading for market speculation, a ‘Reminiscences of a Stock Operator’ for the thinking man.  And with changes in how the global financial system became more political, men like Niederhoffer just don’t seem to be around anymore. Maybe they don’t exist anywhere in this limited, unnatural and politically charged environment. Because Victor Niederhoffer, one of the singularly most impressive people you may find, blew his hedge fund up in 1997 in a highly statistically improbable event, in which he sold puts that were targeted by market mechanics, rather than ‘truth’.  A new layer had been added to the Keynesian beauty contest that is global markets, in which leveraged speculators could now make the market so volatile that the entire equity market could change trajectory over a period of months to target rational, risky and forward thinking positions. Normally the type of technical work that the market completed when Niederhoffer sold his puts would prove the future direction of the market so that a speculator could reasonably presume that they were taking a strong risk-adjusted position. The trade setup that emerged in this new type of market - which I call ‘the second bite of the apple’ -is a disadvantage to people who understand markets and understand risk. It takes a professional back to school and adds a new technical layer to fundamental analysis. The market here is more leveraged, more volatile, more aggressive, better for types of trading and worse for investing. The fundamentals no longer matter; and this was demonstrated only 5 years later with what could be the largest bubble of viable assets in history was formed – the NASDAQ bubble in which household blue chip stocks today lost 98% of their value. This was new. It destroyed Niederhoffer, who was quoted by the Washington Post on the day   ‘ “I’ve made that trade hundreds of times in the past 15 years” he whispered “I believe it’s a good trade. It was a one in 2,000 shot that the market would decline like that” ‘ And it baffled many sophisticated market participants for a long time afterwards. But we can actually see a change in the market! When we consider the S&P500 before and after this event (compare A with B), we can see a fundamental change in the market structure, especially in volatility, and how aggressively the market would trade (look at the second squeeze that would almost put his positions offside a second time – if he had them!). Greenspans moral hazard of late 1994/1995 fundamentally changed the market in a new way. Extraordinary people who were never wrong in risk-adjusted terms and who understood markets were carried out of them horizontally by players with leverage, who knew your position, who were more predatory in nature, had no idea about which direction the market headed in, did not care which direction the market ultimately headed in and the people who actually understood and cared about the big picture were now, irrelevant. They would leverage a short position to 500% of the total market capitalisation of the company they were selling simply to crush it into oblivion, and were able to because of a new reality in the global financial system and a new structure in markets. Financial markets were now more poker – positioning, position size and leverage – and less fundamental – for example, what are the future cash-flows of this business? Does this matter? There is a slice of the population, and not a small slice, who would blindly exchange a Niederhoffer for the more savage trader who profited from his demise any day of the week.  Both are rich, successful and perceptibly in the same dirty business, and for a lot of people they might be thrown into the same basket. Why do they care if a hedge fund manager blows up? Well, because we submit that a new epoch of financial markets had begun. We now had a post-modernist, post-truth financial system headed by a mob characterised by a view of their own power and control in which they could manifest truth and reality as they saw fit – but mostly for their mates on the inside. Of course this was fine for the people creating the financial system, because rationalising your own status, your own righteousness in the use of power and your ability to make earth-changing decisions for the benefits of your powerful friends would feel as if things were the way they should be. Why would they care about some hedge-fund managers positions? He knows the game he was getting himself into and should be a generally astute individual, expert in perceiving and managing risk. But this new financial elite establishment DID care about hedge-fund managers losses, just that it was only specific hedge-fund managers that they cared about. In 1998, Long Term Capital Management (LTCM) busted during the 1998 Russian financial crises, losing up to $4.6 billion USD in face value of an approximately $1 billion USD fund. LTCM was founded by a former vice-chairman of Salomon Brothers (of legendary fame in bonds) and their board included Myron S. Scholes and Robert C. Merton of Black-Scholes fame, who won the Nobel Memorial Prize in Economic Sciences in 1997 for their revolutionary equation in options pricing. Their strategy was extremely highly leveraged (debt to equity over 25:1) trading to lock markets up with theoretical modelling overlays onto the real-world market reality.   At face-value LTCM relied on expertise, but in practice it relied on size and reputation of the people involved, evidenced by its catastrophic failure. To cut a long story short, Alan Greenspans Federal Reserve brokered a deal to rescue LTCM with the benefactor of that deal being Goldman Sachs, who had access to the assets sold at firesale – again a benefactor of the interventions of public institutions filled with their own alumni. The Federal Reserve provided $3.6 billion USD of recapitalisation. For Victor Niederhoffer there is some circular logic that emerges here, that spites and gaslights. LTCM was a group of insiders using incredible amounts of leverage to make wrong decisions in markets. Their existence is precipitated in the dangerous new world of Greenspans Federal Reserve, yet a world in which they ultimately fail on paper, are bailed out in practice.   It spites Niederhoffer because the impact on the market as a result of their failure is the best piece of evidence that Victor Niederhoffer was ultimately wrong when the positions that he took that ended his career would have been threatened again in 1998 in this asset price collapse (his positions were challenged again in 1998, validating the argument that the aggressive, leveraged, even vicious targeting of his positions was natural after all).  But this irresponsible group, who were wrong, and who would go on to invalidate Niederhoffer, who would have been right before this fundamental change in reality and market structure, were then bailed-out, while he began rebuilding his career brick by brick. With these two famous moral hazards – of the Tequila Crises and LTCM bailout, the qualitative features of global financial markets fundamentally change. The big-picture thinkers who understand macro-level knowledge in a unique and sophisticated form give way to aggressive, predatory, leveraged, profit taking machinery.  The result had been written in, in 1995. 1995-2008 was one epoch As time went on, events in 1995 and 1998 became the Nasdaq bubble, the Federal Reserve policy response to the Nasdaq bubble became the housing bubble that led into the global financial crisis. The Global Financial Crisis of 2008 was a result of Greenspans put money and the financial institutions writing the rules for financial institutions. Bear Sterns was allowed to fail and Goldman Sachs got bailout money. Macro-investors like Niederhoffer were never less successful around this time onward, and Victor Niederhoffer blew his fund up a second time during this period. Market speculation was now more purely a highly leveraged, volatile and aggressive game in which fundamentals essentially did not matter. The politically connected were able to exploit event risk, with insider knowledge providing superior returns. The Federal Reserve now provided returns. 2008-2020 was another epoch After 25 years of this phenomenon creeping into global markets, in 2020, the public were ahead of it for the first time. During the coronavirus collapse, aggressive retail investors bought, perhaps for the first time, assets from mid-tier but professional asset managers at the low and continued purchasing until the same mid-tier managers were forced to buy back from retail significantly higher. This was, of course, in unison with aggressive professionals in investment banks (We saw this in the 2020 financial results of these firms) including Goldman Sachs and J.P. Morgan (this co-operation of position may also be the case with GameStop). Now in 2021 we have a new epoch: Game Stop. On the one side we have a faceless mob.  Internet-going redditors at r/wallstreetbets with an expertise in meme magic pit themselves against the out of position hedge-funds that include Stephen Cohen’s Point72, Andrew Left’s Citron Capital, Alex Flynn’s Melvin Capital, Dan Sunheim’s D1 Capital Partners, Maplelane Capital and Candlestick Capital Management and with a total value of something like $55 billion USD. The story behind gamestop is that retail investors calculated a vulnerability in market structure – not unlike the firms and traders who put Niederhoffer out of business – They saw that the highly leveraged, aggressive short position of a handleful of many billion dollar hedge funds were out of position, over-committed to the liquidity and leverage of what they were attempting. In short, the short-selling machine was wrong under the rules of the game that were established just before Victor Niederhoffer sold the family silverware and mortgaged his home to survive. The new rules would now be turned against those who had created and used them for so long.  Part of the strategy uses short-dated options, in which options traders and market makers must cover their positions, where significant leverage means that a $500 position might allow up to hundreds of thousands of dollars in short term exposure < 1 or 2 weeks, and that as the market gets closer to that price, the options market maker must purchase the full amount of the underlying asset of the options. Adding to the confusion is that the fact that the majority of these trades are through a broker called ‘RobinHood’. Established in 2013, RobinHood once published a tweet that said ‘Let the people trade! Robinhood is ‘free’ because it sells its order-flow to high frequency trading firms, with its largest partner being Citadel, a hedge fund.  Citadel has covered Melvin Capitals losses, one of the losing hedge funds in GameStop with upto $3 billion, along with the fund Point72. With Citadel financially supporting the losing hedge-funds, RobinHood banned trade in the affected stocks in the Thursday, 28th January trading session. Where it is literally named after a man who stole from the rich to give to the poor, Robinhood has stolen from the poor to give to the rich. Now there is a rumour that they require funding, and that there are serious problems in the broker-dealer network. But perhaps the bigger problem is that the cynicism of Alan Greenspan, Bill Clinton and Robert Rubin having a meeting in the white-house and deciding to spend public money to bail-out a failed Goldman Sachs deal in Mexico has now spread to the entirety of global financial market participants. The market leverage, volatility, aggression and instability that saw Victor Niederhoffer lose his fund has been normalised.  There are no fundamentals. The game, as it turns out, is pure game.  The response from r/WallStreetBets on reddit is part joker and part Count of Monte Cristo. There is little fear of experiencing losses, and they are interested in punitively claiming a moral victory while using the systems logic against it. Profit is a motive, but is almost a second tier motive. And the response from the institutional side has been, pathetic. Discord banned their r/WallStreetBets chat, RobinHood shut down trading in the assets affected, the Nasdaq said they were ‘monitoring social media’, and there was apoplexy from the establishment financial services, with hedge fund billionaire Leon Cooperman - who had previously been prosecuted for insider trading and came to an agreement with the SEC -  complained about retail traders getting ‘checks from the government’ (Yes, a hedge fund manager complaining that other people were getting government help) responding to a call to tax wealth as ‘a bullshit concept. It’s just a way of attacking wealthy people, and I think it’s inappropriate’. Our sober conclusion is that the global financial system may simply not survive this total moral failure of making it so plainly clear that two sets of rules exist, with this contradiction exposed at a very vulnerable time. If we are correct, and the broken structure of these markets continues to create opportunities that damage risk models, force major losses, corrupt the integrity of brokers, condemn the retail investor and cause major volatility events, we expect more and more from the financial media, whimpering about the problems that they have promoted, participated in and exploited, a broken set of over-leveraged markets weighted towards insiders, dependent on government handouts, that buries a Victor Niederhoffer while rewarding the destruction of the global market pricing mechanism.  And it’s only when the average man on the street participates in their game with their rules, that we hear a complaint, half whimper, half anger. The collective voices in the financial media carrying water for the billionaires who lost money in the game that they perpetuated,  ‘Game, Stop!’ By Thomas Kuhn, CFA  Closing Notes Back to Victor on GameStop We circle back to Victor Niederhoffer, who writes in a blog almost daily. His post is reminiscent of his own experiences. https://wsj.com/articles/the-day-trading-barbarians-at-the-gate-wont-sack-wall-street-11611660505… it isn\\'t cricket used to provide nutrients for infrastructure. Public to provide food for those on higher trophic levels as bacon would say \" the public has no rite to be ahead of the form especially when the trainers are betting against them the horse that were shipped from Florida to NY. are now getting accustomed to track. https://sites.google.com/view/casinoreviews/blog/the-ideology-behind-online-casino-games… first and foremost rule \" the House Must Win\". Its not cricket the public is winning , some top feeders are losing. . the Fed is part of the infrastructure. and they have to do something to stop the public from winning. expect them to beg forbearance from fellow colleagues at Treasury and House and to do something bear. There must be a winning team in south of NY (wall street) . action must be taken. high frequency trading must continue apace,  so however, a temporary cessation of bullish profits for the public will be enough to extend reliance on agrarian ideas. A very sagacious man tweets that a agrarian woman will be yelling. I disagree. one thing we can all agree on is that the be all and end all is for the trophic levels, the sharks to eat the bass —- to be upheld even if two days may lose some gamester support. But the most telling part of Victors post that day, was the end. Victor is now 77 years old, with, as we described, many of those years spent in battle with the house, battling the Fed, battling trophic levels in which LTCM is bailed out and he goes under, seeing how the system works from a humiliating defeat, that there must be a winning team in wall street. But he has something more important to worry about now. ‘Don\\'t expect any pointed posts from me tomorrow( i try to keep them innocuous as I lose a google of followers every time i stray from the Woke.) I have 5 health procedures scheduled for tomorrow culminating in the draining of my left leg which is bigger than Game Stop rise. https://youtube.com/watch?v=Bf4-SBA2Ed8…’ and a conclusion that could be very foreboding ‘The time to stop this infernal topsy turvy market where the bags are temporarily losing to the public. and the serial correlation that supports this in part. But remember old turkey partridges adman. ITs a bull market. after two days look for bull and yet. if it goes down too much before the announcement , a stern meeting at the camp Kinsella woodworking commune may be scheduled in order to varnish a few words.’ Get well soon, Victor. Previously published at https://www.monetaryprotestant.com/post/does-the-game-stop-with-gamestop Create your free account to unlock your custom reading experience.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Chunk Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chunk_length = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end of sentence tag to split on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = ARTICLE.replace('.', '.<eos>')\n",
    "ARTICLE = ARTICLE.replace('?', '?<eos>')\n",
    "ARTICLE = ARTICLE.replace('!', '!<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ARTICLE.split('<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_chunk = 0 \n",
    "chunks = []\n",
    "for sentence in sentences:\n",
    "    if len(chunks) == current_chunk + 1: \n",
    "        if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk_length:\n",
    "            chunks[current_chunk].extend(sentence.split(' '))\n",
    "        else:\n",
    "            current_chunk += 1\n",
    "            chunks.append(sentence.split(' '))\n",
    "    else:\n",
    "        chunks.append(sentence.split(' '))\n",
    "        \n",
    "for chunk_id in range(len(chunks)):\n",
    "    chunks[chunk_id] = ' '.join(chunks[chunk_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Summarize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sign in A Bayesian Take On Model Regularization Ryan Sander Feb 1·10 min read I’m currently reading “How We Learn” by Stanislas Dehaene.   First off, I cannot recommend this book enough to anyone interested in learning, teaching, or AI.   One of the main themes of this book is explaining the neurological and psychological bases of why humans are so good at learning things quickly and with great sample-efficiency, i.  e.   given only a limited amount of experience¹.   One of Dehaene’s main arguments of why humans can learn so effectively is because we are able to reduce the complexity of models we formulate of the world.   In accordance with the principle of Occam’s Razor², we find the simplest model possible that explains the data we experience, rather than opting for more complicated models.   But why do we do this, even from birth¹?   One argument is that, contrary to the frequentist view in child psychology (the belief that babies learn solely through their experiences), we are already imparted with prior beliefs about the world when we are born¹.   This notion of simplified model selection has a common name in the field of machine learning: model regularization.   In this article, we’ll talk about regularization from a Bayesian perspective.   What’s one way we can control the complexity of the models we learn from observations?   We can do this by placing a prior on our distribution of models.   Before we show this, let’s briefly go over regularization, in this case, analytic regularization for supervised learning.   Background on Regularization In machine learning, regularization, or model complexity control, is an essential and common practice to ensure that a model attains high out-of-sample performance, even if the distribution of out-of-sample data (test/validation data) differs significantly from the distribution of in-sample data (training data).   In essence, the model must balance having a small empirical loss (how “wrong” it is on the data it is given) with a small regularization loss (how complicated the model is).   In supervised learning, regularization is usually accomplished via L2 (Ridge)⁸, L1 (Lasso)⁷, or L2/L1 (ElasticNet)⁹ regularization.   For neural networks, there are also techniques such as Drop-out³ or Early Stopping⁴.   For now, we will focus on analytical regularization techniques, since their Bayesian interpretation is more well-defined.   These techniques are summarized below.   Let’s start by defining our dataset and parameters: Next, for a given supervised learning problem in which we wish to minimize a loss function (e.  g.   Mean-Squared Error): Then we have the following objectives for each type of analytical supervised regularization techniques: Below is a comparative plot showing the norm plots of Lasso, ElasticNet, and Ridge regularization, each drawn over a unit sphere.   The code to generate this plot can be found in the Appendix. ',\n",
       " ' In summary, these regularization techniques accomplish different objectives for controlling the complexity of our models.   In the next section, we will derive these regularized objectives by imposing a prior belief (in the form of a probability distribution) on our model parameters, thus directly making the link between prior beliefs and model regularization.   Model Regularization as a Prior Belief on Models Let’s dive deeper into the probabilistic and optimization theory behind implementing regularization through a prior belief in our model parameters.   Specifically, we will demonstrate that: We will analyze these claims for regression problems, but they extend to other supervised learning tasks, such as classification, as well.   We’ll focus on rigorously presenting the mathematics behind these claims (you can also find these derivations in this document here).   Let’s dive in!   L2 (Ridge) Regularization as a Multivariate Gaussian Prior Suppose we again have the same set of observations D and a parameter vector w that we want to optimize in order to best make predictions on samples from D: Using the Maximum a Posteriori (MAP) rule, we can show that the mean and mode of the posterior distribution of w is the solution for ridge regression when we invoke a Gaussian prior distribution on w.   We first invoke Bayes’ Rule: We now define our prior and observation model distributions, with the following assumptions: a.   Prior Model (distribution over parameters w): b.   Observation Model (conditional distribution of observations D conditioned on parameters): Now, let’s substitute these expressions into Bayes’ Rule: To derive our L2 regularized estimator, we now use the MAP rule and the negative log-likelihood function to transform this expression over products into an expression over sums.   This operation is permissible because: (i) The logarithm of the objective is a strictly monotonic transformation of the likelihood function, and thus taking the maximum argument of the log-likelihood function will preserve the cardinality of the likelihood objective and yield the same maximizing argument.   (ii) The maximizing argument of any objective J(θ) is the same as the minimizing argument of the negative objective -J(θ).   Therefore, the argument w* that maximizes the log-likelihood will minimize the negative log-likelihood: Substituting our posterior distribution into our expression for negative log-likelihood: Since logarithms transform products into sums, we can decompose this logarithm of products into a sum of summation terms that depend on our parameters w, along with constants that do not depend on w.   We can then use logarithm rules to simplify this expression.   Removing terms that don’t depend on our parameters w, multiplying the expression by a constant σ², we obtain: The setting of λ = σ² / τ yields that our MAP estimator is also the estimator obtained via ridge regression (when our data is centered around 0): This corresponds exactly to our ridge objective (for regression) above! ',\n",
       " ' In closed-form, this yields the normal equations: Where the λI term is used to ensure that the matrix to be inverted (X^T X) is positive semi-definite, and therefore invertible.   Therefore, placing a Multivariate Gaussian prior on our parameters is equivalent to regularizing our parameters with an L2-norm penalty.   We’ll now examine a similar case with a Laplace prior.   Suppose again that we have a dataset of observations D and a parameter vector w that we want to optimize in order to best make predictions on samples from D: Again using the MAP rule, we can show that the mean and mode of the posterior distribution of w is the solution for LASSO regression when we invoke a Gaussian prior distribution on w.   We first invoke Bayes’ Rule: We now define our prior and observation model distributions, with the following assumptions: a.   Prior Model (distribution over parameters w): b.   Observation Model (Same distribution as above): Repeating the same steps as above (referenced below): Here is the corresponding derivation for Lasso: (Note that in the last step, we set p = 1, q = 1, and λ = α_1 / α_2.  ) As before, we have derived our Lasso objective (for regression) as described in the section above!   Why Does This Matter?   Though it may seem like all we did was invoke some tricks with optimization, logarithms, and probability distributions, the significance of the above derivations may be best understood in the context of the aforementioned novel “How We Learn”.   If I, a human being, am learning a model of the world, why do I tend towards the simplest model that explains my observations¹?   One reason for this is because our brain creates “prior beliefs” over the models we learn even before we learn them, which we take into account when we learn from experience.   Rather than learning these models only from experience, however, we use experience to update our previous beliefs of these models.   By placing a prior belief that the models we learn must be as simple as possible, we are able to control the complexity of the models we learn even before we learn them!   This is exactly what we have done with our analytic derivations above: by placing a prior belief on the distribution of our model parameters (i.  e.   “the model parameters w are normally-distributed”) we are able to directly shape how complex these models are.   Summary In this article, we introduced the ideas of model complexity and regularization, and how these concepts relate to the idea of prior beliefs.   We made the claim that we can control the complexity of a model, i.  e.   regularize it, by setting a prior belief on our distribution of parameters. ',\n",
       " \" We then briefly introduced some common analytical, supervised regularization techniques (Ridge, Lasso, and ElasticNet regression).   We then showed how we can derive the objective functions for Ridge and Lasso regularization using Multivariate Gaussian and Laplace prior distributions, respectively.   Finally, we talked about why these results are significant not only for machine learning, but for psychology as well.   Thanks for reading :) Please follow me for more articles in reinforcement learning, computer vision, programming, and optimization!   Also, a huge thank you to CODECOGS for their online equation rendering tool!   It’s very helpful and easy to use if you want to render mathematics in your Medium articles.   References [1] Dehaene, Stanislas.   How we learn: Why brains learn better than any machine… for now.   Penguin, 2020.   [2] Rasmussen, Carl Edward, and Zoubin Ghahramani.   “Occam’s razor.  ” Advances in neural information processing systems (2001): 294–300.   [3] Srivastava, Nitish, et al.   “Dropout: a simple way to prevent neural networks from overfitting.  ” The journal of machine learning research 15.  1 (2014): 1929–1958.   [4] Caruana, Rich, Steve Lawrence, and Lee Giles.   “Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping.  ” Advances in neural information processing systems (2001): 402–408.   [5] Tibshirani, Robert.   “Regression shrinkage and selection via the lasso.  ” Journal of the Royal Statistical Society: Series B (Methodological) 58.  1 (1996): 267–288.   [6] Calvetti, Daniela, and Lothar Reichel.   “Tikhonov regularization of large linear problems.  ” BIT Numerical Mathematics 43.  2 (2003): 263–283.   [7] Tibshirani, Robert.   “Regression shrinkage and selection via the lasso.  ” Journal of the Royal Statistical Society: Series B (Methodological) 58.  1 (1996): 267–288.   [8] Hoerl, Arthur E.  , and Robert W.   Kennard.   “Ridge regression: Biased estimation for nonorthogonal problems.  ” Technometrics 12.  1 (1970): 55–67.   [9] Zou, Hui, and Trevor Hastie.   “Regularization and variable selection via the elastic net.  ” Journal of the royal statistical society: series B (statistical methodology) 67.  2 (2005): 301–320.   Appendix Code to generate plots (adapted from this Stack OverFlow post).   Graduate Research Assistant @ MIT CSAIL, Tutor, Dark Roast Coffee Drinker, GitHub: https://github.  com/rmsander/, LinkedIn: https://www.  linkedin.  com/in/rmsander 319  1  Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss.  \\xa0Take a look.    By signing up, you will create a Medium account if you don’t already have one.   Review our Privacy Policy for more information about our privacy practices.   Check your inboxMedium sent you an email at  to complete your subscription. \",\n",
       " ' 319\\xa0 319  1  Your home for data science.   A Medium publication sharing concepts, ideas and codes.   About Help Legal Get the Medium app']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = summarizer(chunks, max_length=120, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary_text': ' In machine learning, regularization is an essential and common practice to ensure that a model attains high out-of-sample performance . We can do this by placing a prior on our distribution of models . We will focus on analytical regularization techniques for supervised learning .'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" In machine learning, regularization is an essential and common practice to ensure that a model attains high out-of-sample performance . We can do this by placing a prior on our distribution of models . We will focus on analytical regularization techniques for supervised learning .  These regularization techniques accomplish different objectives for controlling the complexity of our models . In the next section, we will analyze these claims for regression problems, but they extend to other supervised learning tasks, such as classification . We’ll focus on rigorously presenting the mathematics behind these claims .  We’ll now examine a similar case with a Laplace prior . Placing a Gaussian prior on our parameters is equivalent to regularizing our parameters with an L2-norm penalty . The significance of the above derivations may be best understood in the context of “How We Learn”  We briefly introduced some common analytical, supervised regularization techniques (Ridge, Lasso, and ElasticNet regression) We then showed how we can derive objective functions for Ridge and Lasso regularization using Multivariate Gaussian and Laplace prior distributions . Finally, we talked about why these results are significant not only for machine learning, but for psychology as well .  A Medium publication sharing concepts, ideas and codes . Help Legal Get the Medium app . Use this week's weekly Newsquiz to test your knowledge of stories you saw in the news .\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([summ['summary_text'] for summ in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join([summ['summary_text'] for summ in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Output to Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('blogsummary.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "List = [['a', 'b', 'c'], ['a', 'b', 'c'], ['a', 'b', 'c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a2b2c1a2b2c1a2b2c'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1'.join(['2'.join(l) for l in List])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_becode",
   "language": "python",
   "name": "nlp_becode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
